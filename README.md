
DATA ENGINEERING ON AWS

Introduction

This is a project that involves building a platform and pipeline on AWS for Data Engineering.

CONTENTS
- [The Data Set](#the-data-set)
- [Used Tools](#used-tools)
- [Pipelines](#pipelines)
- [Stream Processing](#stream-processing)
- [Storing Data Stream](#storing-data-stream)
- [Processing Data Stream](#processing-data-stream)
- [Batch Processing](#batch-processing)
- [Visualizations](#visualizations)


# The Data Set
- an e-commerce data set from kaggle was used for this project because it is nice to work with.
- the main goal is to build a platform and pipelines with the data set 

# Used Tools
- S3 storage, DynamoDB, Redshift Datawarehouse.

# Pipelines
- Data ingestion (API, Lambda,Kinesis)
- Stream To Raw S3 Storage
- Visualization API For DynamoDB
- Stream to Redshift
- Batch processing pipeline

# Stream Processing
Firstly I created lambda function for kinesis,  also create tha API gateway using resource and methods, configure the API gateway to Lambda function.

# Storing Data Stream
for storing data stream i created the Kinesis data stream called API DATA,IAM was already generated by Lambda.

# Processing Data Stream
S3 bucket and IAM was created for S3, Lambda function was also created to write kinesis to S3
# Batch Processing
a customer and invoice table was created on DynamoDB, an IAM was also created
## Visualizations

# Demo
- You could add a demo video here
- Or link to your presentation video of the project

# Conclusion
Write a comprehensive conclusion.
- How did this project turn out
- What major things have you learned
- What were the biggest challenges

